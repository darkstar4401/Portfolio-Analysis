{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class Portfolio():\n",
    "    def __init__(self,syms,sym_data):\n",
    "        self.syms = syms\n",
    "        self.sym_data = sym_data\n",
    "\n",
    "class QTrader(object):\n",
    "    def __init__(self, sym_data, eta=1, transaction_cost=0.005, position=10):\n",
    "        self.sym_data = sym_data\n",
    "        self.stock_data = sym_data#pd.merge(pd.read_csv('./tbill.csv', index_col='Date'), pd.read_csv('./^GSPC.csv', index_col='Date'), right_index=True, left_index=True).sort_index()\n",
    "        ret = self.stock_data['Close'].rolling(window=2, center=False).apply(lambda x: x[1] / x[0] - 1)\n",
    "        # These are price returns and the weekly returns for TBills (3 months)self.stock_data['Close']\n",
    "        self.returns = pd.DataFrame({\n",
    "                                        'stocks': self.stock_data['Close'].rolling(window=2, center=False).apply(lambda x: x[1] / x[0] - 1),\n",
    "                                        'tbills': (ret.std()) ,\n",
    "                                    }, index=self.stock_data.index)\n",
    "\n",
    "        self.returns['risk_adjusted'] = (self.returns.stocks - self.returns.tbills)\n",
    "        self.returns['risk_adjusted_moving'] = self.returns.risk_adjusted.rolling(window=12).apply(lambda x: x.mean())\n",
    "        self.returns['risk_adjusted_stdev'] = self.returns.risk_adjusted.rolling(window=12).apply(lambda x: x.std())\n",
    "        self.returns['risk_adjusted_high'] = self.returns.risk_adjusted_moving + 1.5 * self.returns.risk_adjusted_stdev\n",
    "        self.returns['risk_adjusted_low'] = self.returns.risk_adjusted_moving - 1.5 * self.returns.risk_adjusted_stdev\n",
    "        self.returns['state'] = (self.returns.risk_adjusted > self.returns.risk_adjusted_high).astype('int') - \\\n",
    "                                (self.returns.risk_adjusted < self.returns.risk_adjusted_high).astype('int') # pd.qcut(self.returns.sharpe_moving, 10, labels=range(10))\n",
    "    def graph_portfolios(self):\n",
    "\n",
    "        midpoint = int(len(self.returns.index) / 2)\n",
    "        training_indexes = self.returns.index[:midpoint] \n",
    "        testing_indexes = self.returns.index[midpoint:]\n",
    "\n",
    "        portfolios = pd.DataFrame({\n",
    "            'buy_and_hold': self.buy_and_hold(testing_indexes),\n",
    "            'buy_tbills': self.buy_tbills(testing_indexes),\n",
    "            'random': self.random(testing_indexes),\n",
    "            'qtrader': self.q_holdings(training_indexes, testing_indexes)\n",
    "            }, index=testing_indexes)\n",
    "\n",
    "        portfolio_values = pd.DataFrame({\n",
    "                'buy_and_hold': self.evaluate(portfolios.buy_and_hold),\n",
    "                'buy_tbills': self.evaluate(portfolios.buy_tbills),\n",
    "                'random': self.evaluate(portfolios.random),\n",
    "                'qtrader': self.evaluate(portfolios.qtrader)\n",
    "            }, index=testing_indexes)\n",
    "        \n",
    "        portfolio_values.plot()\n",
    "\n",
    "        plt.annotate(\"Buy and hold sharpe: {}\\n QTrader sharpe: {}\".format(self.sharpe(portfolios.buy_and_hold), self.sharpe(portfolios.qtrader)), xy=(0.25, 0.95), xycoords='axes fraction')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    def buy_and_hold(self, dates):\n",
    "        return pd.Series(1, index=dates)\n",
    "\n",
    "    def buy_tbills(self, dates):\n",
    "        return pd.Series(0, index=dates)\n",
    "\n",
    "    # This will go long or short or do nothing randomly\n",
    "    def random(self, dates):\n",
    "        return pd.Series(np.random.randint(-1, 2, size=len(dates)), index=dates)\n",
    "\n",
    "    def calc_returns(self, holdings):\n",
    "        return pd.Series(self.returns.tbills + holdings * self.returns.risk_adjusted, index=holdings.index)\n",
    "\n",
    "    def evaluate(self, holdings):\n",
    "        return (self.calc_returns(holdings) + 1).cumprod()\n",
    "\n",
    "    def state(self, first_moment, second_moment):\n",
    "        return first_moment + second_moment * 10\n",
    "\n",
    "    def q_holdings(self, training_indexes, testing_indexes):\n",
    "        factors = pd.DataFrame({'action': 0, 'reward': 0, 'state': 0}, index=training_indexes)\n",
    "\n",
    "        # Initialize Q matrix\n",
    "        q = {0: {1:0, 0:0, -1:0}}\n",
    "        \n",
    "        # For Dyna-Q\n",
    "        T = np.zeros((3, 3, 3)) + 0.00001\n",
    "        R = np.zeros((3,3))\n",
    "\n",
    "        # Episodes\n",
    "        for i in range(100):\n",
    "            last_row, last_date = None, None\n",
    "\n",
    "            for date, row in factors.iterrows():\n",
    "                return_data = self.returns.loc[date]\n",
    "                if return_data.state not in q:\n",
    "                    q[return_data.state] = {1: 0, 0:0, -1:0}\n",
    "\n",
    "                if last_row is None or np.isnan(return_data.state):\n",
    "                    state = 0\n",
    "                    reward = 0\n",
    "                    action = 0\n",
    "                else:\n",
    "                    state = int(return_data.state)\n",
    "                    if random.random() > 0.001:\n",
    "                        action = max(q[state], key=q[state].get)\n",
    "                    else:\n",
    "                        action = random.randint(-1,1)\n",
    "\n",
    "                    reward = last_row.action * (return_data.stocks - return_data.tbills)\n",
    "\n",
    "                    alpha = 1\n",
    "                    discount = 0.9\n",
    "\n",
    "                    factors.loc[date, 'reward'] = reward\n",
    "                    factors.loc[date, 'action'] = action\n",
    "                    factors.loc[date, 'state'] = return_data.state\n",
    "\n",
    "                    update = alpha * (factors.loc[date, 'reward'] + discount * max(q[row.state].values()) - q[state][action])\n",
    "                    if not np.isnan(update):\n",
    "                        q[state][action] += update\n",
    "\n",
    "                    # Dyna\n",
    "                    action_idx = int(last_row.action+1)\n",
    "                    state_idx = int(last_row.state+1)\n",
    "                    new_state_idx = int(state+1)\n",
    "\n",
    "                    T[state_idx][action_idx][new_state_idx] += 1\n",
    "                    R[state_idx][action_idx] = (1 - alpha) * R[state_idx][action_idx] + alpha * reward\n",
    "\n",
    "                last_date, last_row = date, factors.loc[date]\n",
    "\n",
    "            for j in range(100):\n",
    "                state_idx = random.randint(0,2)\n",
    "                action_idx = random.randint(0,2)\n",
    "                new_state = np.random.choice([-1, 0, 1], 1, p=T[state_idx][action_idx]/T[state_idx][action_idx].sum())[0]\n",
    "                r = R[state_idx][action_idx]\n",
    "                q[state][action] += alpha * (r + discount * max(q[new_state].values()) - q[state][action])\n",
    "\n",
    "            sharpe = self.sharpe(factors.action)\n",
    "\n",
    "            if sharpe > 0.20:\n",
    "                break\n",
    "            #print(\"For episode {} we get an internal sharpe ratio of {}\".format(i, self.sharpe(factors.action)))\n",
    "\n",
    "        testing = pd.DataFrame({'action': 0, 'state': 0}, index=testing_indexes)\n",
    "        testing['state'] = self.returns.loc[testing_indexes, 'state']\n",
    "        testing['action'] = testing['state'].apply(lambda state: max(q[state], key=q[state].get))\n",
    "\n",
    "        print(self.sharpe(testing.action))\n",
    "\n",
    "        return testing.action\n",
    "\n",
    "    def discretize(self, number, steps):\n",
    "        return 0\n",
    "\n",
    "    def sharpe(self, holdings):\n",
    "        returns = holdings * (self.returns.stocks - self.returns.tbills)\n",
    "\n",
    "        return np.nanmean(returns) / np.nanstd(returns)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/catalyst-venv/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import datetime\n",
    "from time import sleep\n",
    "from binance.client import Client\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import coint\n",
    "\n",
    "#from crontab import CronTab\n",
    "from safe_schedule import SafeScheduler\n",
    "#add your api keys here\n",
    "api_key = 'XuozVlFg2WYVemwmHMqCK9j5jSYIV1z3MoDAXY76X2JiCbbgcw7dGxvE0eGVsqhH'\n",
    "secret_key = 'WyWBCdczLCFvV8fKy16R5dOwhhjK31T7ZrW0cplIzjrvmlSPh5E1KEeAEh5c9I6s'\n",
    "\n",
    "#Open location of recorded buy/sell walls\n",
    "bw_location = '/home/me/Documents/BWA/data/watcher/buywalls/'\n",
    "sw_location = '/home/me/Documents/BWA/data/watcher/sellwalls/'\n",
    "save_port = '/githome/me/Documents/BWA/data/bw_portfolios'\n",
    "\n",
    "#Open binance client\n",
    "client = Client(api_key, secret_key)\n",
    "\n",
    "#get current time and format to compare with data\n",
    "current_milli_time = lambda: int(round(time.time() * 1000))\n",
    "\n",
    "\n",
    "def get_data(sym,prev_k,k,candle_k='5m'):\n",
    "    \"\"\"\n",
    "    Function downloads hist data from binance API with given timestamps\n",
    "    \"\"\"\n",
    "    #previous kline interval\n",
    "    prev_k = str(prev_k)\n",
    "    #current kline interval\n",
    "    k = str(k)\n",
    "    #download historical data\n",
    "    data = client.get_historical_klines(sym,interval= candle_k,start_str =int(prev_k),end_str = int(k))\n",
    "    data = pd.DataFrame(data,columns=['Open time','Open','High','Low','Close','Volume','Close time','Quote volume','Num trades', 'btc buys', 'coin buys', 'ignore'])\n",
    "    \n",
    "    # convert from millesecond time to datetime for potential resampling and readability\n",
    "    data['Open time'] = data['Open time']/1000\n",
    "    data['Open time'] = data['Open time'].astype('datetime64[s]')\n",
    "    data = data.set_index('Open time')\n",
    "    data = data[['Open','High','Low','Close','Close time','Volume']].astype(float)\n",
    "    return data\n",
    "\n",
    "def to_dt(v,is_milli=True):\n",
    "    \"\"\"\n",
    "    Function to convert timestamp to datetime object\n",
    "    has parameter for millisecond uts timestamp\n",
    "    \"\"\"\n",
    "    if(is_milli):\n",
    "        v = datetime.datetime.fromtimestamp(int(v)/1000)\n",
    "    else:\n",
    "        v = datetime.datetime.fromtimestamp(int(v))\n",
    "    return v\n",
    "\n",
    "def get_freq_sig(loc):\n",
    "    \"\"\"\n",
    "    Function loads all signal csv files and concatenates them into a signle dataframe\n",
    "    \"\"\"\n",
    "    # cd cmd\n",
    "    os.chdir(bw_location)\n",
    "    #sorting by file creation (could use normal sort as filenames are timestamps)\n",
    "    files = filter(os.path.isfile,os.listdir(bw_location))\n",
    "    files =  [int(f) for f in files]\n",
    "    files = list(files)\n",
    "    #Create frequency signal dataframe\n",
    "    fs_df = pd.DataFrame(files)\n",
    "    #sort again\n",
    "    fs_df = fs_df[0].sort_values()\n",
    "    #starttime,endtime\n",
    "    st,end = to_dt(fs_df.iloc[0]),to_dt(fs_df.iloc[-1])\n",
    "    bw_freq = []\n",
    "    \n",
    "    f_len = int(len(os.listdir(loc)))\n",
    "    for c,i in enumerate(os.listdir(loc)):\n",
    "        try:\n",
    "            curr = pd.read_csv(loc+i)\n",
    "        except Exception as e:\n",
    "            print(i,e)\n",
    " \n",
    "        curr.columns = ['Coin','Close','Profit','Loss','Date']  \n",
    "        bw_freq.append(curr)\n",
    "        if((c/f_len)%10==0):\n",
    "            pct = str(int((c/f_len)*100))\n",
    "            print(\"{}% of files loaded\".format(pct))\n",
    "    bw_freq = pd.concat(bw_freq)\n",
    "    bw_freq = bw_freq[['Date','Coin','Close']]\n",
    "    bw_freq.columns = ['Date','Coin','Close']\n",
    "    bw_freq = bw_freq.sort_values(['Date'],ascending=False)\n",
    "    #print(all_w.head(), len(all_w))\n",
    "    #bw_freq['all_freq'] = bw_freq.groupby('Coin')['Coin'].transform('count')\n",
    "    #bw_freq = bw_freq.sort_values(['Date'],ascending=False)    \n",
    "    print(st,end)\n",
    "    print(to_dt(now), now)\n",
    "    print(fs_df.head())\n",
    "    return bw_freq\n",
    "\n",
    "def clean_fs(fs):\n",
    "    \"\"\"\n",
    "    Function to clean frequency signal dataframe\n",
    "    \"\"\"\n",
    "    fs =fs.drop_duplicates()\n",
    "    fs = fs.sort_values(['Date'],axis=0)\n",
    "    fs['Date_m'] = fs['Date']\n",
    "    fs['Date'] = fs['Date']/1000\n",
    "    fs['Date'] = fs['Date'].astype('datetime64[s]')\n",
    "\n",
    "    rolling_f = {}\n",
    "    f_col = []\n",
    "    \"\"\"\n",
    "    for i in fs.iterrows():\n",
    "        coin = i[1][1]\n",
    "        if(coin in rolling_f.keys()):\n",
    "            rolling_f[coin] += 1\n",
    "            f_col.append(rolling_f[coin])\n",
    "        else:\n",
    "            rolling_f[coin] = 1\n",
    "            f_col.append(rolling_f[coin])\n",
    "    print(len(fs),len(f_col))\n",
    "    fs['rolling_freq'] = f_col\n",
    "    \"\"\"    \n",
    "    return fs\n",
    "\n",
    "\n",
    "\n",
    "def unix_time_millis(dt):\n",
    "    \"\"\"\n",
    "    Function to convert unix time to millesecond\n",
    "    \"\"\"\n",
    "    epoch = datetime.datetime.utcfromtimestamp(0)\n",
    "    return (dt - epoch).total_seconds() * 1000.0\n",
    "\n",
    "\n",
    "\n",
    "def interval(intv,st,end):\n",
    "    \"\"\"\n",
    "    Function to resample frequency dataframe to specified frequency\n",
    "    \n",
    "    \"\"\"\n",
    "    intv_list = []\n",
    "    out = pd.DataFrame()\n",
    "    prev = st\n",
    "    while(prev<end):\n",
    "        curr = prev+int(86400000*intv)\n",
    "        n = to_dt(curr).strftime('%Y-%m-%d')\n",
    "        p = to_dt(prev).strftime('%Y-%m-%d')\n",
    "        \n",
    "        print(p,\"  |  \",n)\n",
    "        \n",
    "        prev = curr\n",
    "        p = datetime.datetime.strptime(p, '%Y-%m-%d')\n",
    "        n = datetime.datetime.strptime(n, '%Y-%m-%d')\n",
    "        intv_list.append(p)\n",
    "        data = fs_c.loc[p:n]\n",
    "        #print(data.sort_values('rolling_freq',ascending=False))\n",
    "        out = pd.concat([out,data],axis=0)\n",
    "        \n",
    "    return out.sort_index(),intv_list\n",
    "    #files.sort(key=lambda x: os.path.getmtime(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portfolio interval: 7\n",
      "2018-08-24 00:00:00 2018-09-04 00:00:00\n",
      "2018-09-04 00:00:00 2018-09-11 00:00:00\n",
      "2018-09-11 00:00:00 2018-09-18 00:00:00\n",
      "2018-09-18 00:00:00 2018-09-25 00:00:00\n",
      "2018-09-25 00:00:00 2018-10-02 00:00:00\n",
      "2018-10-02 00:00:00 2018-10-09 00:00:00\n",
      "2018-10-09 00:00:00 2018-10-16 00:00:00\n",
      "2018-10-16 00:00:00 2018-10-23 00:00:00\n",
      "2018-10-23 00:00:00 2018-10-30 00:00:00\n",
      "2018-10-30 00:00:00 2018-11-06 00:00:00\n",
      "2018-11-06 00:00:00 2018-11-13 00:00:00\n",
      "2018-11-13 00:00:00 2018-11-23 00:00:00\n",
      "2018-11-23 00:00:00 2018-11-30 00:00:00\n",
      "2018-11-30 00:00:00 2018-12-07 00:00:00\n",
      "2018-12-07 00:00:00 2018-12-14 00:00:00\n",
      "2018-12-14 00:00:00 2018-12-21 00:00:00\n",
      "2018-12-21 00:00:00 2018-12-28 00:00:00\n",
      "2018-12-28 00:00:00 2019-01-04 00:00:00\n",
      "2019-01-04 00:00:00 2019-01-11 00:00:00\n",
      "2019-01-11 00:00:00 2019-01-18 00:00:00\n",
      "2019-01-18 00:00:00 2019-01-25 00:00:00\n",
      "2019-01-25 00:00:00 2019-02-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def intv_port(freq_df,intv):\n",
    "    dates = freq_df.index.get_level_values('Date').drop_duplicates()\n",
    "    coins = freq_df.index.get_level_values('Coin')\n",
    "    date_int = pd.DataFrame({\"Date\":[d.strftime('%Y-%m-%d') for d in dates]})\n",
    "    date_int = date_int.drop_duplicates()\n",
    "    date_int = date_int.astype('datetime64[s]')\n",
    "    intv_dates = list(date_int.Date[::intv])\n",
    "    port = pd.DataFrame()\n",
    "    for c,day in enumerate(intv_dates):\n",
    "        try:\n",
    "            st,end = day,intv_dates[c+1]\n",
    "            print(st,end)\n",
    "            curr = freq_df.loc[st:end].reset_index()\n",
    "            curr['Date'] = end\n",
    "\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            break\n",
    "\n",
    "        curr['all_freq'] = curr.groupby('Coin')['Coin'].transform('count')\n",
    "        curr = curr[['Date','Coin','Close','all_freq']].sort_values(\"all_freq\",ascending=False)\n",
    "        curr = curr.sort_values(['Date'],ascending=False)\n",
    "        curr = curr.set_index('Date')\n",
    "        port_coins = curr.Coin.drop_duplicates()\n",
    "        c_group = curr.groupby('Coin')\n",
    "        curr_port = pd.DataFrame()\n",
    "        for pc in port_coins:\n",
    "            curr = c_group.get_group(pc).head(1)\n",
    "            curr_port = pd.concat([curr_port,curr])\n",
    "            curr_port = curr_port.sort_values(\"all_freq\",ascending=False).head(9)\n",
    "        port = pd.concat([port,curr_port])\n",
    "    \n",
    "    port = port.reset_index()\n",
    "    #print(port)\n",
    "    port['Date'] = port['Date'].astype('datetime64[ns]')\n",
    "    #print(port)\n",
    "    #print(port['Date'])\n",
    "    d2 = [d.strftime('%Y-%m-%d') for d in port.Date]\n",
    "    port['Date'] = d2\n",
    "    #\n",
    "    return port\n",
    "\n",
    "#get freq. signal df\n",
    "freq_loc = '/home/me/Documents/BWA/data/freq_concat.csv'\n",
    "fs = pd.read_csv(freq_loc)\n",
    "#clean freq. signal df\n",
    "fs_c = clean_fs(fs)\n",
    "#multilevel indexing for interval tracking\n",
    "fs_c = fs_c.set_index(['Date','Coin'])\n",
    "str_intv = input(\"portfolio interval: \")\n",
    "intv = int(str_intv)\n",
    "port = intv_port(fs_c,intv)\n",
    "#print(port)\n",
    "port = port.sort_values(['Date','all_freq'],ascending=[True,False])\n",
    "port = port.set_index(['Date','Coin'])\n",
    "port.to_csv(\"/home/me/Documents/BWA/data/{}day_portfolio.csv\".format(intv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>all_freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Coin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">2019-02-01</th>\n",
       "      <th>ENGBTC</th>\n",
       "      <td>0.000076</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CVCBTC</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MDABTC</th>\n",
       "      <td>0.000215</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APPCBTC</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GXSBTC</th>\n",
       "      <td>0.000155</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUBBTC</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REPBTC</th>\n",
       "      <td>0.003401</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KMDBTC</th>\n",
       "      <td>0.000181</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POWRBTC</th>\n",
       "      <td>0.000024</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Close  all_freq\n",
       "Date       Coin                       \n",
       "2019-02-01 ENGBTC   0.000076       148\n",
       "           CVCBTC   0.000014       147\n",
       "           MDABTC   0.000215       146\n",
       "           APPCBTC  0.000013       145\n",
       "           GXSBTC   0.000155       140\n",
       "           SUBBTC   0.000012       123\n",
       "           REPBTC   0.003401       116\n",
       "           KMDBTC   0.000181       115\n",
       "           POWRBTC  0.000024        97"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port.iloc[-9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio lookback window2\n",
      "Index(['2019-01-25', '2019-02-01'], dtype='object', name='Date')\n",
      "2019-01-25 00:00:00 2019-02-01 00:00:00\n",
      "Fitting the rolling Linear Regression...\n",
      "                         feature1\n",
      "Open time                        \n",
      "2019-01-25 03:15:00  1.642654e-09\n",
      "2019-01-25 03:20:00  1.365511e-09\n",
      "2019-01-25 03:25:00  1.373702e-09\n",
      "2019-01-25 03:30:00  9.411924e-10\n",
      "2019-01-25 03:35:00  4.254458e-10\n",
      "1978 1978\n",
      "                      APPCBTC  BTCUSDT   hedge_ratio\n",
      "Open time                                           \n",
      "2019-01-25 03:15:00  0.000013  3570.22  1.642654e-09\n",
      "2019-01-25 03:20:00  0.000013  3572.31  1.365511e-09\n",
      "2019-01-25 03:25:00  0.000013  3572.44  1.373702e-09\n",
      "2019-01-25 03:30:00  0.000013  3571.39  9.411924e-10\n",
      "2019-01-25 03:35:00  0.000013  3570.78  4.254458e-10\n",
      "Creating the spread/zscore columns...\n",
      "z avg:  -1.3315169099872778e-15\n",
      "z std:  1.0002528764738867\n",
      "z max:  4.993635392191146\n",
      "z min:  -3.1658090891621176\n",
      "                      APPCBTC  BTCUSDT   hedge_ratio    spread    zscore  \\\n",
      "Open time                                                                  \n",
      "2019-01-25 03:15:00  0.000013  3570.22  1.642654e-09  0.000008  0.007031   \n",
      "2019-01-25 03:20:00  0.000013  3572.31  1.365511e-09  0.000009  0.059093   \n",
      "2019-01-25 03:25:00  0.000013  3572.44  1.373702e-09  0.000008  0.053596   \n",
      "2019-01-25 03:30:00  0.000013  3571.39  9.411924e-10  0.000010  0.134261   \n",
      "2019-01-25 03:35:00  0.000013  3570.78  4.254458e-10  0.000012  0.226782   \n",
      "\n",
      "                     longs  shorts  exits  long_market  short_market  \n",
      "Open time                                                             \n",
      "2019-01-25 03:15:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 03:20:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 03:25:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 03:30:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 03:35:00    0.0     0.0    0.0          0.0           0.0  \n",
      "Calculating when to be in the market (long and short)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/catalyst-venv/lib/python3.6/site-packages/ipykernel_launcher.py:113: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      APPCBTC  BTCUSDT   hedge_ratio        spread    zscore  \\\n",
      "Open time                                                                      \n",
      "2019-01-25 03:15:00  0.000013  3570.22  1.642654e-09  7.525365e-06  0.007031   \n",
      "2019-01-25 03:20:00  0.000013  3572.31  1.365511e-09  8.561970e-06  0.059093   \n",
      "2019-01-25 03:25:00  0.000013  3572.44  1.373702e-09  8.452531e-06  0.053596   \n",
      "2019-01-25 03:30:00  0.000013  3571.39  9.411924e-10  1.005863e-05  0.134261   \n",
      "2019-01-25 03:35:00  0.000013  3570.78  4.254458e-10  1.190083e-05  0.226782   \n",
      "2019-01-25 03:40:00  0.000013  3570.53 -4.204419e-11  1.357012e-05  0.310619   \n",
      "2019-01-25 03:45:00  0.000013  3567.74 -4.898666e-10  1.516772e-05  0.390856   \n",
      "2019-01-25 03:50:00  0.000013  3568.50 -6.372800e-10  1.569413e-05  0.417295   \n",
      "2019-01-25 03:55:00  0.000013  3568.06 -9.248711e-10  1.672000e-05  0.468817   \n",
      "2019-01-25 04:00:00  0.000013  3567.92 -1.284751e-09  1.801389e-05  0.533801   \n",
      "2019-01-25 04:05:00  0.000013  3568.15 -1.989359e-09  2.052833e-05  0.660085   \n",
      "2019-01-25 04:10:00  0.000013  3568.94 -2.666642e-09  2.294709e-05  0.781563   \n",
      "2019-01-25 04:15:00  0.000013  3570.44 -2.972975e-09  2.405483e-05  0.837197   \n",
      "2019-01-25 04:20:00  0.000013  3570.09 -3.333550e-09  2.533107e-05  0.901295   \n",
      "2019-01-25 04:25:00  0.000013  3569.91 -4.017677e-09  2.778274e-05  1.024426   \n",
      "2019-01-25 04:30:00  0.000013  3569.92 -5.043944e-09  3.144648e-05  1.208431   \n",
      "2019-01-25 04:35:00  0.000013  3569.96 -5.560277e-09  3.328997e-05  1.301018   \n",
      "2019-01-25 04:40:00  0.000013  3570.61 -5.807701e-09  3.417703e-05  1.345569   \n",
      "2019-01-25 04:45:00  0.000013  3570.55 -5.998918e-09  3.485944e-05  1.379842   \n",
      "2019-01-25 04:50:00  0.000013  3570.00 -6.033576e-09  3.497987e-05  1.385890   \n",
      "2019-01-25 04:55:00  0.000013  3570.66 -5.710858e-09  3.383153e-05  1.328217   \n",
      "2019-01-25 05:00:00  0.000013  3568.89 -5.392906e-09  3.268669e-05  1.270719   \n",
      "2019-01-25 05:05:00  0.000013  3564.11 -5.297563e-09  3.235110e-05  1.253865   \n",
      "2019-01-25 05:10:00  0.000013  3559.93 -5.144855e-09  3.178532e-05  1.225449   \n",
      "2019-01-25 05:15:00  0.000013  3563.30 -4.691370e-09  3.010676e-05  1.141146   \n",
      "2019-01-25 05:20:00  0.000013  3559.67 -3.700165e-09  2.656137e-05  0.963084   \n",
      "2019-01-25 05:25:00  0.000013  3560.83 -3.241028e-09  2.494075e-05  0.881692   \n",
      "2019-01-25 05:30:00  0.000013  3561.76 -3.068186e-09  2.435814e-05  0.852431   \n",
      "2019-01-25 05:35:00  0.000013  3564.00 -2.961056e-09  2.398320e-05  0.833600   \n",
      "2019-01-25 05:40:00  0.000013  3566.26 -2.889625e-09  2.373515e-05  0.821142   \n",
      "...                       ...      ...           ...           ...       ...   \n",
      "2019-01-31 21:35:00  0.000012  3438.41  7.860342e-09 -1.523708e-05 -1.136177   \n",
      "2019-01-31 21:40:00  0.000012  3437.01  8.251075e-09 -1.656903e-05 -1.203072   \n",
      "2019-01-31 21:45:00  0.000012  3438.22  8.151059e-09 -1.623514e-05 -1.186303   \n",
      "2019-01-31 21:50:00  0.000012  3438.26  8.309903e-09 -1.680161e-05 -1.214753   \n",
      "2019-01-31 21:55:00  0.000012  3441.94  8.406069e-09 -1.716318e-05 -1.232913   \n",
      "2019-01-31 22:00:00  0.000012  3437.65  8.352263e-09 -1.692216e-05 -1.220808   \n",
      "2019-01-31 22:05:00  0.000012  3440.48  7.756086e-09 -1.489466e-05 -1.118980   \n",
      "2019-01-31 22:10:00  0.000012  3446.00  7.388538e-09 -1.367090e-05 -1.057518   \n",
      "2019-01-31 22:15:00  0.000012  3447.77  6.788463e-09 -1.161506e-05 -0.954267   \n",
      "2019-01-31 22:20:00  0.000012  3446.56  6.343150e-09 -1.007205e-05 -0.876771   \n",
      "2019-01-31 22:25:00  0.000012  3446.64  5.369701e-09 -6.717428e-06 -0.708291   \n",
      "2019-01-31 22:30:00  0.000012  3448.98  5.183439e-09 -6.037578e-06 -0.674147   \n",
      "2019-01-31 22:35:00  0.000012  3448.46  3.498982e-09 -2.960998e-07 -0.385790   \n",
      "2019-01-31 22:40:00  0.000012  3447.60  2.276374e-09  3.961971e-06 -0.171935   \n",
      "2019-01-31 22:45:00  0.000012  3443.63  1.770294e-09  5.713764e-06 -0.083954   \n",
      "2019-01-31 22:50:00  0.000012  3446.18  1.274962e-09  7.346252e-06 -0.001965   \n",
      "2019-01-31 22:55:00  0.000012  3445.65  7.526460e-10  9.146645e-06  0.088457   \n",
      "2019-01-31 23:00:00  0.000012  3442.00 -5.708985e-10  1.373503e-05  0.318902   \n",
      "2019-01-31 23:05:00  0.000012  3434.51 -5.324843e-10  1.358882e-05  0.311559   \n",
      "2019-01-31 23:10:00  0.000012  3435.83  8.375912e-11  1.149222e-05  0.206260   \n",
      "2019-01-31 23:15:00  0.000012  3433.51  4.090885e-10  1.037539e-05  0.150169   \n",
      "2019-01-31 23:20:00  0.000012  3433.31  8.665870e-10  8.804738e-06  0.071286   \n",
      "2019-01-31 23:25:00  0.000012  3435.31  8.925106e-10  8.713950e-06  0.066726   \n",
      "2019-01-31 23:30:00  0.000012  3434.06  1.296489e-09  7.287781e-06 -0.004901   \n",
      "2019-01-31 23:35:00  0.000012  3431.01  1.399617e-09  6.957899e-06 -0.021469   \n",
      "2019-01-31 23:40:00  0.000012  3429.41  1.457532e-09  6.761525e-06 -0.031332   \n",
      "2019-01-31 23:45:00  0.000012  3426.76  1.345614e-09  7.148903e-06 -0.011876   \n",
      "2019-01-31 23:50:00  0.000012  3431.62  1.667993e-09  5.996083e-06 -0.069775   \n",
      "2019-01-31 23:55:00  0.000012  3434.10  1.667719e-09  6.042887e-06 -0.067424   \n",
      "2019-02-01 00:00:00  0.000012  3435.82  1.659391e-09  6.068632e-06 -0.066131   \n",
      "\n",
      "                     longs  shorts  exits  long_market  short_market  \n",
      "Open time                                                             \n",
      "2019-01-25 03:15:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 03:20:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 03:25:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 03:30:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 03:35:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 03:40:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 03:45:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 03:50:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 03:55:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 04:00:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 04:05:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 04:10:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 04:15:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 04:20:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 04:25:00    0.0     1.0    0.0          0.0           0.0  \n",
      "2019-01-25 04:30:00    0.0     1.0    0.0          0.0           0.0  \n",
      "2019-01-25 04:35:00    0.0     1.0    0.0          0.0           0.0  \n",
      "2019-01-25 04:40:00    0.0     1.0    0.0          0.0           0.0  \n",
      "2019-01-25 04:45:00    0.0     1.0    0.0          0.0           0.0  \n",
      "2019-01-25 04:50:00    0.0     1.0    0.0          0.0           0.0  \n",
      "2019-01-25 04:55:00    0.0     1.0    0.0          0.0           0.0  \n",
      "2019-01-25 05:00:00    0.0     1.0    0.0          0.0           0.0  \n",
      "2019-01-25 05:05:00    0.0     1.0    0.0          0.0           0.0  \n",
      "2019-01-25 05:10:00    0.0     1.0    0.0          0.0           0.0  \n",
      "2019-01-25 05:15:00    0.0     1.0    0.0          0.0           0.0  \n",
      "2019-01-25 05:20:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 05:25:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 05:30:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 05:35:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-25 05:40:00    0.0     0.0    0.0          0.0           0.0  \n",
      "...                    ...     ...    ...          ...           ...  \n",
      "2019-01-31 21:35:00    1.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 21:40:00    1.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 21:45:00    1.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 21:50:00    1.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 21:55:00    1.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 22:00:00    1.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 22:05:00    1.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 22:10:00    1.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 22:15:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 22:20:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 22:25:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 22:30:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 22:35:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 22:40:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 22:45:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 22:50:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 22:55:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 23:00:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 23:05:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 23:10:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 23:15:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 23:20:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 23:25:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 23:30:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 23:35:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 23:40:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 23:45:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 23:50:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-01-31 23:55:00    0.0     0.0    0.0          0.0           0.0  \n",
      "2019-02-01 00:00:00    0.0     0.0    0.0          1.0           1.0  \n",
      "\n",
      "[1978 rows x 10 columns]\n",
      "2019-02-01 00:00:00 2019-02-01 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the rolling Linear Regression...\n",
      "  File \"<ipython-input-21-30d208c042e5>\", line 47, in <module>\n",
      "    rolling = ols.PandasRollingOLS(y=pairs[symbols[0]], x=pairs[symbols[1]], window=40)\n",
      "data already contains a constant\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ffn\n",
    "from pyfinance import ols\n",
    "import traceback, sys\n",
    "#loop test thresholds\n",
    "z_entry_threshold=1.\n",
    "z_exit_threshold=-.8\n",
    "    \n",
    "p_dates = port.index.get_level_values('Date').drop_duplicates()\n",
    "n_day = int(input(\"Portfolio lookback window\"))\n",
    "last_n = p_dates[-n_day:]\n",
    "print(last_n)\n",
    "returns = []\n",
    "all_ports = pd.DataFrame()\n",
    "for c,d in enumerate(last_n):\n",
    "\n",
    "    if(c>len(last_n)-1):\n",
    "        break\n",
    "    try:\n",
    "        st = datetime.datetime.strptime(d, '%Y-%m-%d')\n",
    "        if(len(last_n)==2):\n",
    "            end = datetime.datetime.strptime(last_n[-1], '%Y-%m-%d')\n",
    "        else:\n",
    "            end = datetime.datetime.strptime(last_n[c+1], '%Y-%m-%d')\n",
    "        print(st,end)\n",
    "        \n",
    "        st,end = int(unix_time_millis(st)),int(unix_time_millis(end))\n",
    "        prev_st = st-86400000*intv\n",
    "        curr_port = port.loc[d]\n",
    "        p_coins = curr_port.index.get_level_values('Coin').values\n",
    "        curr_port_hist = pd.DataFrame()\n",
    "        for coin in p_coins:\n",
    "            #current hist ...lazy way\n",
    "            #q_hist = get_data(coin,prev_st,end,candle_k='1h')\n",
    "            #q trader hist with prior data for training\n",
    "            #coin hist\n",
    "            c_hist = get_data(coin,st,end,candle_k='5m')\n",
    "            #btc hist\n",
    "            btc_hist = get_data('BTCUSDT',st,end,candle_k='5m')\n",
    "            symbols = [coin,'BTCUSDT']\n",
    "            #intraday pairs df\n",
    "            pairs = pd.DataFrame(index=c_hist.index)\n",
    "            pairs[symbols[0]] = c_hist['Close'].values\n",
    "            pairs[symbols[1]] = btc_hist['Close'].values\n",
    "            pairs = pairs.dropna()\n",
    "            # linear regression between the two closing price time series\n",
    "            print(\"Fitting the rolling Linear Regression...\")\n",
    "            rolling = ols.PandasRollingOLS(y=pairs[symbols[0]], x=pairs[symbols[1]], window=40)\n",
    "            #model = sm.OLS(y=btc_hist['Close'].values, X=c_hist['Close'].values).fit()\n",
    "            #model = sm.OLS(y=pairs['%s_close' % symbols[0].lower()], \n",
    "                           #X=pairs['%s_close' % symbols[1].lower()],\n",
    "                           #).fit()\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Construct the hedge ratio and eliminate the first \n",
    "            # lookback-length empty/NaN period\n",
    "            #res.params\n",
    "            beta = rolling.beta\n",
    "            print(beta.head())\n",
    "            pairs['hedge_ratio'] = beta\n",
    "            pairs = pairs.dropna()\n",
    "            print(len(beta),len(pairs))\n",
    "            print(pairs.head())\n",
    "            # Create the spread and then a z-score of the spread\n",
    "            print(\"Creating the spread/zscore columns...\")\n",
    "            pairs['spread'] = pairs[symbols[0]] - pairs['hedge_ratio']*pairs[symbols[1]]\n",
    "            pairs['zscore'] = (pairs['spread'] - np.mean(pairs['spread']))/np.std(pairs['spread'])\n",
    "            \n",
    "            print(\"z avg: \", pairs['zscore'].mean())\n",
    "            print(\"z std: \", pairs['zscore'].std())\n",
    "            print(\"z max: \", pairs['zscore'].max())\n",
    "            print(\"z min: \", pairs['zscore'].min())\n",
    "            \"\"\"Create the entry/exit signals based on the exceeding of \n",
    "            z_enter_threshold for entering a position and falling below\n",
    "            z_exit_threshold for exiting a position.\"\"\"\n",
    "\n",
    "            # Calculate when to be long, short and when to exit\n",
    "            pairs['longs'] = (pairs['zscore'] <= -z_entry_threshold)*1.0\n",
    "            pairs['shorts'] = (pairs['zscore'] >= z_entry_threshold)*1.0\n",
    "            pairs['exits'] = (np.abs(pairs['zscore']) <= z_exit_threshold)*1.0\n",
    "\n",
    "            # These signals are needed because we need to propagate a\n",
    "            # position forward, i.e. we need to stay long if the zscore\n",
    "            # threshold is less than z_entry_threshold by still greater\n",
    "            # than z_exit_threshold, and vice versa for shorts.\n",
    "            pairs['long_market'] = 0.0\n",
    "            pairs['short_market'] = 0.0\n",
    "            print(pairs.head())\n",
    "            # These variables track whether to be long or short while\n",
    "            # iterating through the bars\n",
    "            long_market = 0\n",
    "            short_market = 0\n",
    "\n",
    "            # Calculates when to actually be \"in\" the market, i.e. to have a\n",
    "            # long or short position, as well as when not to be.\n",
    "            # Since this is using iterrows to loop over a dataframe, it will\n",
    "            # be significantly less efficient than a vectorised operation,\n",
    "            # i.e. slow!\n",
    "            print(\"Calculating when to be in the market (long and short)...\")\n",
    "            for i, b in enumerate(pairs.iterrows()):\n",
    "                # Calculate longs\n",
    "                if b[1]['longs'] == 1.0:\n",
    "                    long_market = 1            \n",
    "                # Calculate shorts\n",
    "                if b[1]['shorts'] == 1.0:\n",
    "                    short_market = 1\n",
    "                # Calculate exists\n",
    "                if b[1]['exits'] == 1.0:\n",
    "                    long_market = 0\n",
    "                    short_market = 0\n",
    "            # This directly assigns a 1 or 0 to the long_market/short_market\n",
    "            # columns, such that the strategy knows when to actually stay in!\n",
    "            pairs.ix[i]['long_market'] = long_market\n",
    "            pairs.ix[i]['short_market'] = short_market\n",
    "            \n",
    "            print(pairs)\n",
    "            break\n",
    "            # Construct the portfolio object with positions information\n",
    "            # Note that minuses to keep track of shorts!\n",
    "            print(\"Constructing a portfolio...\")\n",
    "            portfolio = pd.DataFrame(index=pairs.index)\n",
    "            portfolio['positions'] = pairs['long_market'] - pairs['short_market']\n",
    "            portfolio[symbols[0]] = -1.0 * pairs[symbols[0]] * portfolio['positions']\n",
    "            portfolio[symbols[1]] = pairs[symbols[1]] * portfolio['positions']\n",
    "            portfolio['total'] = portfolio[symbols[0]] + portfolio[symbols[1]]\n",
    "            \n",
    "            # Construct a percentage returns stream and eliminate all \n",
    "            # of the NaN and -inf/+inf cells\n",
    "            print(\"Constructing the equity curve...\")\n",
    "            portfolio['returns'] = portfolio['total'].pct_change()\n",
    "            portfolio['returns'].fillna(0.0, inplace=True)\n",
    "            portfolio['returns'].replace([np.inf, -np.inf], 0.0, inplace=True)\n",
    "            portfolio['returns'].replace(-1.0, 0.0, inplace=True)\n",
    "\n",
    "            # Calculate the full equity curve\n",
    "            portfolio['returns'] = (portfolio['returns'] + 1.0).cumprod()\n",
    "            portfolio['total'].plot()\n",
    "            #portfolio.returns.plot()\n",
    "            plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "            break\n",
    "            \n",
    "            \n",
    "            #qb_hist = get_data('BTCUSDT',prev_st,end,candle_k='1h')\n",
    "            \"\"\"\n",
    "            #this one works needs testing\n",
    "            q_hist['Stdev'] = q_hist['Close'].rolling(window=12).std()\n",
    "            q_hist['Moving Average'] = q_hist['Close'].rolling(window=24).mean()\n",
    "            q_hist['Criteria1'] = (q_hist['Open'] - q_hist['Low'].shift(1)) < -q_hist['Stdev']\n",
    "            q_hist['Criteria2'] = q_hist['Open'] > q_hist['Moving Average']\n",
    "            q_hist['BUY'] = q_hist['Criteria1'] & q_hist['Criteria2']\n",
    "            q_hist['Pct Change'] = (q_hist['Close'] - q_hist['Open']) / q_hist['Open']\n",
    "            q_hist['Returns'] = q_hist['Pct Change'][q_hist['BUY'] == True]\n",
    "            q_hist['Returns'].fillna(0).plot()\n",
    "            \n",
    "            curr_port_hist[coin] = q_hist['Returns'].values\n",
    "            \n",
    "            qt = QTrader(q_hist)\n",
    "            print(qt.buy_and_hold(t_hist.index))\n",
    "            #later replace with corr data\n",
    "            midpoint = int(len(qt.returns.index) / 2)\n",
    "            training_indexes = qt.returns.index[:midpoint] \n",
    "            testing_indexes = qt.returns.index[midpoint:]\n",
    "            sym_trade = qt.q_holdings(training_indexes,testing_indexes)\n",
    "            c_hist[\"btc returns\"] = btc_hist['Close']#\n",
    "            c_hist['sym_btc'] = c_hist[\"btc returns\"]*c_hist['Close']\n",
    "            c_hist['sym_btc'] = c_hist['sym_btc'].pct_change() \n",
    "            c_hist['Signal'] = sym_trade\n",
    "            c_hist['cpct'] = c_hist['Close'].pct_change()\n",
    "            c_hist['Market Returns'] = np.log(c_hist['Close'] / c_hist['Close'].shift(1))\n",
    "            c_hist['Strategy'] = c_hist['sym_btc'] * c_hist['Signal']#.shift(1)\n",
    "            #print(c_hist[[]].head(10))\n",
    "            c_hist[['sym_btc','Strategy']].cumsum().plot(grid=True,figsize=(8,5))\n",
    "            #c_hist[['Close','Signal']].plot(subplots=True,grid=True)\n",
    "            #plt.show()\n",
    "            \"\"\"\n",
    "            \n",
    "          \n",
    "\n",
    "        all_ports = pd.concat([all_ports,curr_port_hist],axis=0)\n",
    "        #print(curr_port_hist.head()) \n",
    "        #print(p_coins)\n",
    "        \n",
    "    except Exception as e:\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        traceback.print_tb(exc_traceback, limit=1, file=sys.stdout)\n",
    "        print(e)\n",
    "        \n",
    "        pass\n",
    "all_ports\n",
    "#all_ports= all_ports.fillna(0)\n",
    "#all_ports.to_csv(fname+\"all_coins.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
